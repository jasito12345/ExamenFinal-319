22/12/12 02:55:18 WARN Utils: Your hostname, alvaro-VirtualBox resolves to a loopback address: 127.0.1.1; using 192.168.100.37 instead (on interface enp0s3)
22/12/12 02:55:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/12/12 02:55:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://192.168.100.37:4040
Spark context available as 'sc' (master = local[*], app id = local-1670828147964).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.1
      /_/
         
Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.17)
Type in expressions to have them evaluated.
Type :help for more information.

scala> val cadenas = Array("Docentes", "inteligenciaArtificial", "quefinal")        // Se inicia un Array con tres cadenas
cadenas: Array[String] = Array(Docentes, inteligenciaArtificial, quefinal)

scala> val cadenasRDD = sc . parallelize (cadenas)                                  // Se convierte el array de cadenas en una colección para trabajar en paralelo
cadenasRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala> cadenasRDD.collect()                                                         // Recopilación de los datos de la colección en paralelo
res0: Array[String] = Array(Docentes, inteligenciaArtificial, quefinal)         

scala> file.collect()                                                               // Recopilación de datos de un objeto "file" que no se encuentran o no existen
<console>:23: error: not found: value file
       file.collect()
       ^

scala> val filtro = cadenasRDD.filter(line => line.contains("quefinal"))            // Se busca los elementos de la colección que contengan la cadena "quefinal"
filtro: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at filter at <console>:23

scala> val fileNotFound = sc.textFile("/7añljdlsjd/alkls/", 6)
fileNotFound: org.apache.spark.rdd.RDD[String] = /7añljdlsjd/alkls/ MapPartitionsRDD[3] at textFile at <console>:23

scala> fileNotFound.collect()
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/7añljdlsjd/alkls
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)
  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
  at org.apache.spark.rdd.RDD.collect(RDD.scala:1020)
  ... 47 elided
Caused by: java.io.IOException: Input path does not exist: file:/7añljdlsjd/alkls
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)
  ... 63 more

scala>
